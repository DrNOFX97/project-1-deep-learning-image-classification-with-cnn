{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68fd32f-1b0e-4ff5-aedb-26204bd73e8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Make predictions on a pre-trained model\n",
    "## Key Points:\n",
    "1. Data Loading and Preprocessing: Loads the CIFAR-10 dataset, normalizes the data, and one-hot encodes the labels.\n",
    "2. Model Definition: Loads the pre-trained VGG16 model without the top layer and adds custom dense layers for CIFAR-10 classification.\n",
    "3. Layer Freezing: Freezes all layers of the VGG16 base model to prevent them from being trained (no fine-tuning).\n",
    "4. Model Compilation: Compiles the model with the Adam optimizer and categorical cross-entropy loss.\n",
    "5. Training with Progress Bar: Trains the model with a progress bar for visual feedback.\n",
    "6. Evaluation: Evaluates the model on the test dataset and prints out the performance metrics in a tabulated format.\n",
    "* This code represents a minimalistic approach to transfer learning, where the pre-trained model is used as a feature extractor without any further training of its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb8fe33-a135-491d-808a-b00274e005bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 50016samples [00:31, 1585.76samples/s]                      \n",
      "Epoch 2/10: 50016samples [00:28, 1762.27samples/s]                      \n",
      "Epoch 3/10: 50016samples [00:29, 1710.93samples/s]                      \n",
      "Epoch 4/10: 50016samples [00:29, 1703.15samples/s]                      \n",
      "Epoch 5/10: 50016samples [00:28, 1744.93samples/s]                      \n",
      "Epoch 6/10: 50016samples [00:28, 1730.56samples/s]                      \n",
      "Epoch 7/10: 50016samples [00:28, 1774.29samples/s]                      \n",
      "Epoch 8/10: 50016samples [00:29, 1667.97samples/s]                      \n",
      "Epoch 9/10: 50016samples [00:27, 1791.09samples/s]                      \n",
      "Epoch 10/10: 50016samples [00:27, 1794.21samples/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|       Metric        | Value  |\n",
      "+---------------------+--------+\n",
      "|     Train Loss      | 1.1459 |\n",
      "|   Train Accuracy    | 0.6045 |\n",
      "|   Validation Loss   | 1.1990 |\n",
      "| Validation Accuracy | 0.5856 |\n",
      "|      Test Loss      | 1.1990 |\n",
      "|    Test Accuracy    | 0.5856 |\n",
      "+---------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Load the VGG16 model without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freeze all layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with a progress bar\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Training the model\")\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=len(x_train), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"samples\") as pbar:\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=0,\n",
    "            callbacks=[tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: pbar.update(batch_size))]\n",
    "        )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Extract metrics from the history\n",
    "train_loss = history.history['loss'][-1]\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Prepare data for tabulate\n",
    "table_data = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Train Loss\", f\"{train_loss:.4f}\"],\n",
    "    [\"Train Accuracy\", f\"{train_accuracy:.4f}\"],\n",
    "    [\"Validation Loss\", f\"{val_loss:.4f}\"],\n",
    "    [\"Validation Accuracy\", f\"{val_accuracy:.4f}\"],\n",
    "    [\"Test Loss\", f\"{loss:.4f}\"],\n",
    "    [\"Test Accuracy\", f\"{accuracy:.4f}\"]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e8aa15-ecc6-44d1-aaa8-18849350ce9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n",
      "+-----------+--------+\n",
      "| Precision | 0.5833 |\n",
      "+-----------+--------+\n",
      "|  Recall   | 0.5856 |\n",
      "| F1 Score  | 0.5829 |\n",
      "+-----------+--------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "# Prepare data for tabulate\n",
    "extra_table_data = [\n",
    "    [\"Precision\", f\"{precision:.4f}\"],\n",
    "    [\"Recall\", f\"{recall:.4f}\"],\n",
    "    [\"F1 Score\", f\"{f1:.4f}\"]\n",
    "]\n",
    "\n",
    "# Print the updated table\n",
    "print(tabulate(extra_table_data, headers=\"firstrow\", tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014bcb7-a287-42c5-9f6d-84b55b5ff720",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune the pre-trained model\n",
    "## Key Points:\n",
    "1. **Data Loading and Preprocessing**: Loads the CIFAR-10 dataset, normalizes the data, and one-hot encodes the labels.\n",
    "2. **Model Definition**: Loads the pre-trained VGG16 model without the top layer and adds custom dense layers for CIFAR-10 classification.\n",
    "3. **Layer Freezing**: Initially freezes all layers of the VGG16 base model to prevent them from being trained during the initial training phase.\n",
    "4. **Initial Training**: Trains the model with the base model layers frozen.\n",
    "5. **Layer Unfreezing**: Unfreezes the last 4 layers of the base model for fine-tuning.\n",
    "6. **Fine-Tuning Training**: Trains the model with some of the base model layers unfrozen.\n",
    "7. **Evaluation**: Evaluates the model on the test dataset and prints out the performance metrics in a tabulated format.\n",
    "* This approach allows the model to first learn the new task with the pre-trained weights and then fine-tune some of the deeper layers for potentially better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1bf562-7bee-450f-acb4-6bac199cbe8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model (initially frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 50016samples [00:29, 1724.65samples/s]                      \n",
      "Epoch 2/5: 50016samples [00:28, 1762.14samples/s]                      \n",
      "Epoch 3/5: 50016samples [00:31, 1598.86samples/s]                      \n",
      "Epoch 4/5: 50016samples [00:31, 1594.49samples/s]                      \n",
      "Epoch 5/5: 50016samples [00:30, 1630.63samples/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 50016 samples [00:59, 842.24 samples/s]                     \n",
      "Epoch 2/10: 50016 samples [00:57, 868.37 samples/s]                     \n",
      "Epoch 3/10: 50016 samples [00:57, 868.48 samples/s]                     \n",
      "Epoch 4/10: 50016 samples [00:57, 865.52 samples/s]                     \n",
      "Epoch 5/10: 50016 samples [00:57, 867.71 samples/s]                     \n",
      "Epoch 6/10: 50016 samples [00:58, 860.24 samples/s]                     \n",
      "Epoch 7/10: 50016 samples [00:57, 870.50 samples/s]                     \n",
      "Epoch 8/10: 50016 samples [00:57, 869.06 samples/s]                     \n",
      "Epoch 9/10: 50016 samples [00:57, 868.20 samples/s]                     \n",
      "Epoch 10/10: 50016 samples [00:57, 869.66 samples/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|       Metric        | Value  |\n",
      "+---------------------+--------+\n",
      "|     Train Loss      | 0.2578 |\n",
      "|   Train Accuracy    | 0.9212 |\n",
      "|   Validation Loss   | 0.7874 |\n",
      "| Validation Accuracy | 0.7468 |\n",
      "|      Test Loss      | 0.7874 |\n",
      "|    Test Accuracy    | 0.7468 |\n",
      "+---------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Load the VGG16 model without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model (initially with frozen base model layers)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initial training with frozen base model\n",
    "initial_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Training the model (initially frozen)\")\n",
    "for epoch in range(initial_epochs):\n",
    "    with tqdm(total=len(x_train), desc=f\"Epoch {epoch+1}/{initial_epochs}\", unit=\"samples\") as pbar:\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=0,\n",
    "            callbacks=[tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: pbar.update(batch_size))]\n",
    "        )\n",
    "\n",
    "# Unfreeze some layers for fine-tuning\n",
    "for layer in base_model.layers[-4:]:  # Unfreeze the last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tuning training\n",
    "fine_tuning_epochs = 10\n",
    "\n",
    "print(\"Fine-tuning the model\")\n",
    "for epoch in range(fine_tuning_epochs):\n",
    "    with tqdm(total=len(x_train), desc=f\"Epoch {epoch+1}/{fine_tuning_epochs}\", unit=\" samples\") as pbar:\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=0,\n",
    "            callbacks=[tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: pbar.update(batch_size))]\n",
    "        )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Extract metrics from the history\n",
    "train_loss = history.history['loss'][-1]\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Prepare data for tabulate\n",
    "table_data = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Train Loss\", f\"{train_loss:.4f}\"],\n",
    "    [\"Train Accuracy\", f\"{train_accuracy:.4f}\"],\n",
    "    [\"Validation Loss\", f\"{val_loss:.4f}\"],\n",
    "    [\"Validation Accuracy\", f\"{val_accuracy:.4f}\"],\n",
    "    [\"Test Loss\", f\"{loss:.4f}\"],\n",
    "    [\"Test Accuracy\", f\"{accuracy:.4f}\"]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c434a111-25f6-471e-a61d-d418b5a07260",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step\n",
      "+-----------+--------+\n",
      "| Precision | 0.7450 |\n",
      "+-----------+--------+\n",
      "|  Recall   | 0.7468 |\n",
      "| F1 Score  | 0.7450 |\n",
      "+-----------+--------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "# Prepare data for tabulate\n",
    "extra_table_data = [\n",
    "    [\"Precision\", f\"{precision:.4f}\"],\n",
    "    [\"Recall\", f\"{recall:.4f}\"],\n",
    "    [\"F1 Score\", f\"{f1:.4f}\"]\n",
    "]\n",
    "\n",
    "# Print the updated table\n",
    "print(tabulate(extra_table_data, headers=\"firstrow\", tablefmt=\"pretty\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
